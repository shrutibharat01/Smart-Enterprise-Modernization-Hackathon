{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de149673-b914-4843-b7aa-f0dc0a694c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24b7da8-4324-47e8-9ad4-afc83f89ae6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import boto3\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "SECRET_SCOPE = \"aws_scope\"\n",
    "AWS_KEY_SECRET = \"aws-access-key-id\"\n",
    "AWS_SECRET_SECRET = \"aws-secret-access-key\"\n",
    "AWS_REGION = \"ap-south-1\" \n",
    "\n",
    "S3_BUCKET = \"smart-enterprise-modernization-data\"\n",
    "S3_PREFIX = \"ML_Model_Output_2/\"\n",
    "MLFLOW_EXPERIMENT = \"/Users/bharatshruti02@gmail.com/smart-enterprise-random-forest-model\"\n",
    "REGISTERED_MODEL_NAME = \"smart_enterprise_random_forest_model\"\n",
    "GOLD_TABLE = \"enterprise_modernization.gold.gold_car_sales_analytics\"\n",
    "RANDOM_STATE = 42\n",
    "LOCAL_TMP_DIR = \"/tmp\"\n",
    "\n",
    "os.makedirs(LOCAL_TMP_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- FETCH AWS CREDS FROM SECRET SCOPE ----------\n",
    "access_key = dbutils.secrets.get(scope=\"aws_scope\", key=\"aws-access-key-id\")\n",
    "secret_key = dbutils.secrets.get(scope=\"aws_scope\", key=\"aws-secret-access-key\")\n",
    "\n",
    "# Export to env vars so MLflow & boto3 both pick them up\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = access_key\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = secret_key\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION\n",
    "\n",
    "# ---------- INIT BOTO3 SESSION ----------\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "s3 = session.resource(\"s3\")\n",
    "\n",
    "# Test connection\n",
    "print(\"‚úÖ Connected to S3 bucket:\", S3_BUCKET)\n",
    "for obj in s3.Bucket(S3_BUCKET).objects.limit(1):\n",
    "    print(\"Sample object:\", obj.key)\n",
    "\n",
    "# ---------- LOAD GOLD TABLE ----------\n",
    "print(\"Loading gold table:\", GOLD_TABLE)\n",
    "gold_df = spark.table(GOLD_TABLE).toPandas()\n",
    "print(\"Loaded rows:\", len(gold_df))\n",
    "\n",
    "# ---------- FEATURE SELECTION ----------\n",
    "num_features = [\n",
    "    \"crm_Engine_Size\", \"crm_Vehicle_Age\", \"crm_Mileage\",\n",
    "    \"sap_Net_Sale\", \"fleet_Maintenance_Cost\",\n",
    "    \"fleet_Fuel_Consumption\", \"fleet_Accidents_Count\"\n",
    "]\n",
    "cat_features = [\n",
    "    \"crm_Manufacturer\", \"crm_Model\", \"crm_Fuel_type\",\n",
    "    \"sap_Region\", \"sap_Payment_Mode\", \"fleet_Fleet_Type\"\n",
    "]\n",
    "target_col = \"crm_Price\"\n",
    "\n",
    "missing_cols = [c for c in num_features + cat_features + [target_col] if c not in gold_df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing columns in gold table: {missing_cols}\")\n",
    "\n",
    "gold_df = gold_df[gold_df[target_col].notnull()].copy()\n",
    "X = gold_df[num_features + cat_features]\n",
    "y = gold_df[target_col]\n",
    "\n",
    "# ---------- TRAIN / TEST SPLIT ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# ---------- PREPROCESSING PIPELINE ----------\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Compatible across sklearn versions\n",
    "try:\n",
    "    categorical_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "except TypeError:\n",
    "    categorical_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, num_features),\n",
    "    (\"cat\", categorical_transformer, cat_features)\n",
    "])\n",
    "\n",
    "# ---------- MODEL + GRID ----------\n",
    "rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"estimator\", rf)])\n",
    "\n",
    "param_grid = {\n",
    "    \"estimator__n_estimators\": [100, 200],\n",
    "    \"estimator__max_depth\": [10, 20],\n",
    "    \"estimator__min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring=\"r2\", n_jobs=1, verbose=1)\n",
    "\n",
    "# ---------- MLflow Experiment Setup ----------\n",
    "client = MlflowClient()\n",
    "artifact_location = f\"s3://{S3_BUCKET}/{S3_PREFIX}\"\n",
    "\n",
    "experiment = client.get_experiment_by_name(MLFLOW_EXPERIMENT)\n",
    "if experiment is None:\n",
    "    print(f\"Creating new MLflow experiment: {MLFLOW_EXPERIMENT}\")\n",
    "    exp_id = mlflow.create_experiment(name=MLFLOW_EXPERIMENT, artifact_location=artifact_location)\n",
    "else:\n",
    "    exp_id = experiment.experiment_id\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "\n",
    "# ---------- TRAIN & LOG ----------\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(\"MLflow run id:\", run_id)\n",
    "\n",
    "    # Train\n",
    "    print(\"Starting training with GridSearchCV...\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"R2: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "\n",
    "    # Log metrics and params\n",
    "    mlflow.log_metrics({\"r2\": r2, \"mae\": mae, \"rmse\": rmse})\n",
    "    for k, v in grid.best_params_.items():\n",
    "        mlflow.log_param(k, v)\n",
    "    mlflow.log_param(\"gold_table\", GOLD_TABLE)\n",
    "\n",
    "    # ---------- SAVE LOCAL MODEL ----------\n",
    "    ts = datetime.datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    local_model_path = os.path.join(LOCAL_TMP_DIR, f\"rf_model_{run_id}_{ts}.pkl\")\n",
    "    joblib.dump(best_model, local_model_path)\n",
    "    print(\"‚úÖ Model saved locally at:\", local_model_path)\n",
    "\n",
    "    # ---------- UPLOAD TO S3 ----------\n",
    "    s3_key_latest = f\"{S3_PREFIX}random_forest_model_latest.pkl\"\n",
    "    #s3_key_versioned = f\"{S3_PREFIX}rf_model_{run_id}_{ts}.pkl\"\n",
    "\n",
    "    try:\n",
    "        #s3.Bucket(S3_BUCKET).upload_file(local_model_path, s3_key_versioned)\n",
    "        s3.Bucket(S3_BUCKET).upload_file(local_model_path, s3_key_latest)\n",
    "        print(\"‚úÖ Uploaded model to S3:\")\n",
    "        print(f\" - Latest: s3://{S3_BUCKET}/{s3_key_latest}\")\n",
    "        #print(f\" - Versioned: s3://{S3_BUCKET}/{s3_key_versioned}\")\n",
    "        \n",
    "        \n",
    "        # Save metrics locally\n",
    "        metrics = {\"rmse\": float(rmse), \"mae\": float(mae), \"r2\": float(r2)}\n",
    "        metrics_local_path = os.path.join(\"/tmp\", f\"model_metrics_{ts}.json\")\n",
    "        \n",
    "        with open(metrics_local_path, \"w\") as f:\n",
    "            json.dump(metrics, f)\n",
    "            \n",
    "        print(\"‚úÖ Metrics saved locally at:\", metrics_local_path)\n",
    "        \n",
    "\n",
    "        # Upload Metrics to S3 automatically\n",
    "        metrics_s3_key = f\"{S3_PREFIX}model_metrics.json\"\n",
    "        s3_client = session.client('s3')\n",
    "        s3_client.upload_file(metrics_local_path, S3_BUCKET, metrics_s3_key)\n",
    "\n",
    "        print(\"‚úÖ Uploaded metrics JSON to S3:\")\n",
    "        print(f\" - s3://{S3_BUCKET}/{metrics_s3_key}\")\n",
    "\n",
    "        mlflow.log_param(\"s3_model_latest_uri\", f\"s3://{S3_BUCKET}/{s3_key_latest}\")\n",
    "        mlflow.log_param(\"s3_metrics_uri\", f\"s3://{S3_BUCKET}/{metrics_s3_key}\")\n",
    "        #mlflow.log_param(\"s3_model_versioned_uri\", f\"s3://{S3_BUCKET}/{s3_key_versioned}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Failed to upload to S3:\", e)\n",
    "\n",
    "print(\"üéØ Training complete. Artifacts & metrics logged in MLflow.\")\n",
    "print(f\"üì¶ S3 model path (latest): s3://{S3_BUCKET}/{s3_key_latest}\")\n",
    "print(f\"üìä Metrics JSON path: s3://{S3_BUCKET}/{metrics_s3_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7fe067c-d988-43bd-8188-b3032ffe1f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6425086465330156,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ML_Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
