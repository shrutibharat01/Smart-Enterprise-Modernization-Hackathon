{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a7d27c8-91d1-4d68-bf5c-5e4231352c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d98936-c67e-4188-9d37-9ebed6fa0e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import boto3\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Read the credentials from the table you uploaded\n",
    "creds_df = spark.table(\"delta_lake.default.shruti_hackathon_access_keys\")\n",
    "creds_df.show()  # For debugging: see the structure\n",
    "\n",
    "# Extract credentials as local Python variables\n",
    "creds_row = creds_df.first()\n",
    "access_key = creds_row['Access key ID']\n",
    "secret_key = creds_row['Secret access key']\n",
    "\n",
    "# Create a session with the extracted credentials\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "bucket = s3.Bucket('smart-enterprise-modernization-data')\n",
    "\n",
    "# List objects as a test\n",
    "for obj in bucket.objects.all():\n",
    "    print(obj.key)\n",
    "\n",
    "\n",
    "# Read and prepare data\n",
    "gold_df = spark.table(\"enterprise_modernization.gold.customer_vehicle_fleet\")\n",
    "features = [\"price\", \"engine_size\", \"mileage\", \"fault_count\", \"avg_odometer\"]\n",
    "target = \"sales\"\n",
    "gold_pd = gold_df.select(features + [target]).dropna().toPandas()\n",
    "gold_pd[features] = gold_pd[features].astype(float)\n",
    "X = gold_pd[features]\n",
    "y = gold_pd[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MSE: {mse:.2f}\")\n",
    "print(f\"Test R^2 Score: {r2:.2f}\")\n",
    "\n",
    "# Set experiment with S3 artifact location (do this once)\n",
    "experiment_name = \"/Users/bharatshruti02@gmail.com/vehicle_sales_prediction\"\n",
    "artifact_location = \"s3://smart-enterprise-modernization-data/ML_Model_Output/vehicle_sales_prediction_model\"\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        name=experiment_name,\n",
    "        artifact_location=artifact_location\n",
    "    )\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Log model and metrics\n",
    "with mlflow.start_run() as run:\n",
    "    input_example = X_train.head(10).to_dict(orient='records')[0]\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"random_forest_model\",\n",
    "        input_example=input_example\n",
    "    )\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Register model in Model Registry (versioning is automatic)\n",
    "    mlflow.register_model(\n",
    "        model_uri=f\"runs:/{run.info.run_id}/random_forest_model\",\n",
    "        name=\"vehicle_sales_prediction_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3863aa62-6a29-48b6-9c63-5d63bd04dc10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"vehicle_sales_prediction_model\"\n",
    "model_version = 1\n",
    "\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Load new data\n",
    "new_data_df = spark.read.table(\"enterprise_modernization.gold.customer_vehicle_fleet\")\n",
    "\n",
    "selected_features = [\"price\", \"engine_size\", \"mileage\", \"fault_count\", \"avg_odometer\"]\n",
    "\n",
    "input_df = (new_data_df.select(selected_features).toPandas().dropna().astype(float))\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(input_df)\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions, columns=[\"predicted_sales\"])\n",
    "\n",
    "display(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a64a5d7-6155-454b-8f58-c9421df486b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "instance_url = dbutils.secrets.get(\"my_scope\", \"databricks_instance_url\").rstrip(\"/\")\n",
    "pat_token = dbutils.secrets.get(\"my_scope\", \"databricks_pat_token\")\n",
    "endpoint_name = \"vehicle_sales_prediction_endpoint\"\n",
    "\n",
    "endpoint_url = f\"{instance_url}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {pat_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "input_df = (new_data_df.select(features).toPandas().dropna().astype(float))\n",
    "\n",
    "if not input_df.empty:\n",
    "    payload = {\"dataframe_records\": input_df.to_dict(orient=\"records\")}\n",
    "    response = requests.post(\n",
    "        endpoint_url,\n",
    "        headers=headers,\n",
    "        data=json.dumps(payload)\n",
    "    )\n",
    "    predictions = response.json()\n",
    "    display(predictions)\n",
    "else:\n",
    "    print(\"Input data is empty. No predictions to display.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Prediction_Model_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
